{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TripAdvisor Recomender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection from TripAdvisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data are scraped using Selenium and BeautifulSoup from TripAdvisor website from late April to early May 2019. The hotels are from 16 popular European tourist destination cities and their TripAdvisor rating range from 3.0 to 5.0. The collected data are separated into 4 files. These files are:\n",
    "\n",
    "1) tripadvisor_data.csv (hotel information from the hotel listings from the city section)\n",
    "\n",
    "2) tripadvisor_hotel.txt (hotel information from the hotel details webpage in json format)\n",
    "\n",
    "3) tripadvisor_reviewer.csv (brief reviewer information from the hotel details webpage)\n",
    "\n",
    "4) tripadvisor_hotel_review.csv (reviews written by reviewer from the TripAdvisor reviewer webpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries  \n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup \n",
    "import urllib\n",
    "import requests, re\n",
    "from datetime import datetime\n",
    "import time as t\n",
    "from time import time\n",
    "import os, sys\n",
    "from lxml import html, etree\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from surprise import SVD, SVDpp, SlopeOne, NMF, NormalPredictor, KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore,  BaselineOnly, CoClustering\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise import dump\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping through TripAdvisor City Section for Hotel Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_hotel_list(locality, checkin_date, checkout_date, hotel_data, sort=\"popularity\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        locality is the city location of the hotel search\n",
    "\n",
    "        checkin_date and checkout_date are the start and end date of hotel stay.\n",
    "        It should be year/month/day string format, ie. '2019/07/01'\n",
    "\n",
    "        sort is the method of how search results are displayed either by order of 'popularity', 'value', 'price' or 'distance'\n",
    "\n",
    "        hotel_data is a list used to collect scraped hotel information which will be returned back by the function.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    check_in = checkin_date\n",
    "    check_out = checkout_date\n",
    "    \n",
    "    geo_url = 'https://www.tripadvisor.com/TypeAheadJson?action=API&startTime='+str(int(time()))+'&uiOrigin=GEOSCOPE&source=GEOSCOPE&interleaved=true&types=geo,theme_park&neighborhood_geos=true&link_type=hotel&details=true&max=12&injectNeighborhoods=true&query='+locality\n",
    "    api_response  = requests.get(geo_url, verify=False).json()\n",
    "    url_from_autocomplete = \"http://www.tripadvisor.com\"+api_response['results'][0]['url']\n",
    "    geo = api_response['results'][0]['value'] \n",
    "    date = check_in+\"_\"+check_out\n",
    "    \n",
    "    form_data = {'changeSet': 'TRAVEL_INFO',\n",
    "                'showSnippets': 'false',\n",
    "                'staydates':date,\n",
    "                'uguests': '2',\n",
    "                'sortOrder':sort\n",
    "    \n",
    "                }\n",
    "    \n",
    "    headers = {\n",
    "                'Accept': 'text/javascript, text/html, application/xml, text/xml, */*',\n",
    "                'Accept-Encoding': 'gzip,deflate',\n",
    "                'Accept-Language': 'en-US,en;q=0.5',\n",
    "                'Cache-Control': 'no-cache',\n",
    "                'Connection': 'keep-alive',\n",
    "                'Content-Type': 'application/x-www-form-urlencoded; charset=utf-8',\n",
    "                'Host': 'www.tripadvisor.com',\n",
    "                'Pragma': 'no-cache',\n",
    "                'Referer': url_from_autocomplete,\n",
    "                'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:28.0) Gecko/20100101 Firefox/28.0',\n",
    "                'X-Requested-With': 'XMLHttpRequest'\n",
    "                }\n",
    "    \n",
    "    cookies=  {\"SetCurrency\":\"USD\"}\n",
    "    page_response  = requests.post(url = url_from_autocomplete,data=form_data,headers = headers, cookies = cookies, verify=False)\n",
    "    if page_response.status_code != 200:\n",
    "        print('Page error for, '+ url_from_autocomplete +', is found.')\n",
    "        return hotel_data\n",
    "    else:\n",
    "        parser = html.fromstring(page_response.text)\n",
    "        hotel_data = iterate_hotels(parser, locality, hotel_data)\n",
    "        return hotel_data\n",
    "    \n",
    "    \n",
    "    \n",
    "def iterate_hotels(parser, locality, hotel_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    this function is called by process_hotel_list() in order to parse and scrape hotel information\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    next_page = parser.xpath('//a[contains(@class,\"nav next taLnk ui_button primary\")]/@href')\n",
    "    next_page = 'http://www.tripadvisor.com' + next_page[0].replace('#BODYCON','') if next_page else  None\n",
    "    print(\"Processing content from \" + next_page)\n",
    "    \n",
    "    hotel_lists = parser.xpath('//div[contains(@class,\"listItem\")]//div[contains(@class,\"listing collapsed\")]')\n",
    "    if not hotel_lists:\n",
    "        hotel_lists = parser.xpath('//div[contains(@class,\"listItem\")]//div[@class=\"listing \"]')\n",
    "\n",
    "    for index, hotel in enumerate(hotel_lists): # grab the element of each feature from html\n",
    "        XPATH_HOTEL_LINK = './/a[contains(@class,\"property_title\")]/@href'\n",
    "        XPATH_REVIEWS  = './/a[@class=\"review_count\"]//text()'\n",
    "        XPATH_RANK = './/div[@class=\"popRanking\"]//text()'\n",
    "        XPATH_RATING = './/a[contains(@class,\"ui_bubble_rating\")]//@alt'\n",
    "        XPATH_HOTEL_NAME = './/a[contains(@class,\"property_title\")]//text()'\n",
    "        XPATH_HOTEL_FEATURES = './/div[contains(@class,\"common_hotel_icons_list\")]//li//text()'\n",
    "        XPATH_HOTEL_PRICE = './/div[contains(@data-sizegroup,\"mini-meta-price\")]/text()'\n",
    "        XPATH_VIEW_DEALS = './/div[contains(@data-ajax-preserve,\"viewDeals\")]//text()' \n",
    "        XPATH_BOOKING_PROVIDER = './/div[contains(@data-sizegroup,\"mini-meta-provider\")]//text()'\n",
    "\n",
    "        raw_booking_provider = hotel.xpath(XPATH_BOOKING_PROVIDER)\n",
    "        raw_no_of_deals =  hotel.xpath(XPATH_VIEW_DEALS)\n",
    "        raw_hotel_link = hotel.xpath(XPATH_HOTEL_LINK)\n",
    "        raw_no_of_reviews = hotel.xpath(XPATH_REVIEWS)\n",
    "        raw_rank = hotel.xpath(XPATH_RANK)\n",
    "        raw_rating = hotel.xpath(XPATH_RATING)\n",
    "        raw_hotel_name = hotel.xpath(XPATH_HOTEL_NAME)\n",
    "        raw_hotel_features = hotel.xpath(XPATH_HOTEL_FEATURES)\n",
    "        raw_hotel_price_per_night  = hotel.xpath(XPATH_HOTEL_PRICE)\n",
    "\n",
    "        url = 'http://www.tripadvisor.com'+raw_hotel_link[0] if raw_hotel_link else  None\n",
    "        reviews = ''.join(raw_no_of_reviews).replace(\"reviews\",\"\").replace(\",\",\"\") if raw_no_of_reviews else 0 \n",
    "        rank = ''.join(raw_rank) if raw_rank else None\n",
    "        rating = ''.join(raw_rating).replace('of 5 bubbles','').strip() if raw_rating else None\n",
    "        name = ''.join(raw_hotel_name).strip() if raw_hotel_name else None\n",
    "        hotel_features = ','.join(raw_hotel_features)\n",
    "        price_per_night = ''.join(str(*raw_hotel_price_per_night)).replace('\\n','') if raw_hotel_price_per_night else None\n",
    "        no_of_deals = re.findall(\"all\\s+?(\\d+)\\s+?\",''.join(raw_no_of_deals))\n",
    "        booking_provider = ''.join(raw_booking_provider).strip() if raw_booking_provider else None\n",
    "\n",
    "        if no_of_deals:\n",
    "            no_of_deals = no_of_deals[0]\n",
    "        else:\n",
    "            no_of_deals = 0\n",
    "\n",
    "        data = {\n",
    "                'hotel_name':name,\n",
    "                'url':url,\n",
    "                'locality':locality,\n",
    "                'reviews':reviews,\n",
    "                'tripadvisor_rating':rating,\n",
    "                'hotel_features':hotel_features,\n",
    "                'price_per_night':price_per_night,\n",
    "                'no_of_deals':no_of_deals,\n",
    "                'booking_provider':booking_provider\n",
    "\n",
    "                }\n",
    "        hotel_data.append(data)\n",
    "        \n",
    "    if next_page:\n",
    "        t.sleep(1)\n",
    "    try:\n",
    "        page_response = requests.get(next_page)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        t.sleep(60)\n",
    "        page_response = requests.get(next_page)\n",
    "        parser = html.fromstring(page_response.text)\n",
    "        hotel_data = iterate_hotels(parser, locality, hotel_data)\n",
    "    else:\n",
    "        parser = html.fromstring(page_response.text)\n",
    "        hotel_data = iterate_hotels(parser, locality, hotel_data)\n",
    "    finally:\n",
    "        return hotel_data\n",
    "        \n",
    "    return hotel_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_data = process_hotel_list('London, England', '2019/07/01', '2019/07/15', hotel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write hotel_data to a csv file called tripadvisor_data.csv\n",
    "# with open('tripadvisor_data.csv','wb')as csvfile:  # substitute 'wb' for 'ab' to append hotel_data to existing tripadvisor_data.csv\n",
    "            fieldnames = ['hotel_name','url','locality','reviews','tripadvisor_rating','price_per_night','booking_provider','no_of_deals','hotel_features']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row in hotel_data:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadata = pd.read_csv('tripadvisor_data.csv')\n",
    "tadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadata = tadata.drop_duplicates() # drop duplicate hotels from list\n",
    "# tadata.to_csv(\"tripadvisor_data.csv\", index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping through Individual TripAdvisor Hotel Webpage for Hotel Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_hotel_details(hotel_url, retry=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    hotel_url is hotel webpage at TripAdvisor\n",
    "    retry attempts to reconnect to TripAdvisor if it fails up to MAX_RETRY times\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    MAX_RETRY = 10\n",
    "    RETRY = 0\n",
    "\n",
    "    headers = {\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "                \"accept-encoding\": \"gzip, deflate, br\",\n",
    "                \"accept-language\": \"en-GB,en;q=0.9,en-US;q=0.8,ml;q=0.7\",\n",
    "                \"cache-control\": \"max-age=0\",\n",
    "                \"upgrade-insecure-requests\": \"1\",\n",
    "                \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/71.0.3578.80 Chrome/71.0.3578.80 Safari/537.36\",\n",
    "                }\n",
    "\n",
    "    response = requests.get(hotel_url, headers=headers)\n",
    "    if response.status_code == 404:\n",
    "        print('error: Page not found, status_code: 404')\n",
    "        pass\n",
    "    parser = html.fromstring(response.text, myurl)\n",
    "\n",
    "    XPATH_NAME = '//h1[@id=\"HEADING\"]//text()'\n",
    "    XPATH_RANK = '//span[contains(@class,\"popularity\")]//text()'\n",
    "    XPATH_FULL_ADDRESS_JSON = '//script[@type=\"application/ld+json\"]//text()'\n",
    "\n",
    "    raw_name = parser.xpath(XPATH_NAME)\n",
    "    raw_rank = parser.xpath(XPATH_RANK)\n",
    "    raw_address_json = parser.xpath(XPATH_FULL_ADDRESS_JSON)\n",
    "    name = clean(raw_name)\n",
    "    rank = clean(raw_rank)\n",
    "    if not name:\n",
    "        if RETRY < MAX_RETRY:\n",
    "            RETRY = RETRY+1\n",
    "            # Retrying the same URL\n",
    "            process_hotel_details(hotel_url, RETRY)\n",
    "\n",
    "    hotel_rating = 0\n",
    "    review_count = 0\n",
    "    address = {}\n",
    "    if raw_address_json:\n",
    "        try:\n",
    "            parsed_address_info = json.loads(raw_address_json[0])\n",
    "            rating = parsed_address_info.get('aggregateRating', {})\n",
    "            address = parsed_address_info.get(\"address\", {})\n",
    "\n",
    "            hotel_rating = rating.get('ratingValue')\n",
    "            review_count = rating.get('reviewCount')\n",
    "\n",
    "            address = {\n",
    "                        'street_address': address.get('streetAddress'),\n",
    "                        'region': address.get('addressRegion'),\n",
    "                        'locality': address.get('addressLocality'),\n",
    "                        'country': address.get(\"addressCountry\", {}).get(\"name\"),\n",
    "                        'zipcode': address.get(\"postalCode\")\n",
    "                    }\n",
    "        except Exception as e:\n",
    "            review_count = hotel_rating = 0\n",
    "            raise e\n",
    "\n",
    "    ratings = {}\n",
    "    elems = parser.find_class(\"hotels-review-list-parts-ReviewRatingFilter__row_num--gIW_f\")\n",
    "    if elems:\n",
    "        ratings = {\n",
    "            'Excellent': elems[0].text,\n",
    "            'Good': elems[1].text,\n",
    "            'Average': elems[2].text,\n",
    "            'Poor': elems[3].text,\n",
    "            'Terrible': elems[4].text\n",
    "                }\n",
    "\n",
    "    amenities = parser.find_class(\"hotels-hotel-review-about-with-photos-Amenity__name--2IUMR\")\n",
    "    amenity_list = []     \n",
    "    for a in amenities:\n",
    "        amenity_list.append(a.text)\n",
    "\n",
    "    data = {\n",
    "            'address': address,\n",
    "            'ratings': ratings,\n",
    "            'amenities': amenity_list,\n",
    "            'rating': float(hotel_rating) if hotel_rating else 0.0,\n",
    "            'review_count': int(review_count) if review_count else 0,\n",
    "            'name': name,\n",
    "            'rank': rank,\n",
    "            'hotel_url': hotel_url\n",
    "            }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    processing scraped information before adding to a json formatted list of dictionaries\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if text:\n",
    "        # Removing \\n \\r and \\t\n",
    "        return ' '.join(''.join(text).split()).strip()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_details = []\n",
    "for index, hotel in enumerate(tadata['url'][0:1000]):\n",
    "    result = process_request(hotel)\n",
    "    hotel_details.append(result)\n",
    "    print(index)  # keeping track of progress in case of error\n",
    "    t.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = []\n",
    "details.extend(hotel_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving details list as tripadvisor_hotel.txt in json format\n",
    "# with open('tripadvisor_hotel.txt', 'w') as out_file:\n",
    "    json.dump(details, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tripadvisor_hotel.txt to pandas DataFrame\n",
    "tahotel = pd.read_json('tripadvisor_hotel.txt')\n",
    "tahotel = tahotel[tahotel.review_count >= 100]   # filtering hotels with more than 100 reviews \n",
    "tahotel.index = range(len(tahotel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping through Individual TripAdvisor Hotel Webpage for Guest Review Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_reviews(hotel_url, reviewer_list):\n",
    "    \n",
    "    response = requests.get(hotel_url)\n",
    "    if response.status_code != 200:\n",
    "        print('Error is found on page, '+ hotel_url)\n",
    "        return reviewer_list\n",
    "    else:\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        reviews = soup.findAll(class_ = 'hotels-review-list-parts-SingleReview__reviewContainer--d54T4')\n",
    "        review_count = 10\n",
    "        reviewer_list = iterate_review_page(soup, reviews, review_count, reviewer_list)\n",
    "        return reviewer_list\n",
    "            \n",
    "def iterate_review_page(soup, reviews, review_count, reviewer_list):           \n",
    "\n",
    "    next_page = soup.find('a', class_=\"ui_button nav next primary \", href=True)\n",
    "    next_page = 'http://www.tripadvisor.com'+next_page['href'] if next_page['href'] else  None\n",
    "       \n",
    "    for r in reviews:\n",
    "        try:\n",
    "            contribute = r.find('span', class_ = 'social-member-MemberHeaderStats__bold--3z3qh').get_text()\n",
    "            contribute = int(contribute.replace(',', '')) \n",
    "        except AttributeError:\n",
    "            contribute = 0    \n",
    "            pass\n",
    "        \n",
    "        if contribute > 25 and review_count > 0: #filter reviewer with more than 25 reviews and a limit of 10 reviewers for each hotel\n",
    "            try:\n",
    "                reviewer = r.find('a', class_ = 'ui_header_link social-member-event-MemberEventOnObjectBlock__member--35-jC').get_text()\n",
    "            except AttributeError:\n",
    "                reviewer = \"NONE PROVIDED\"\n",
    "                \n",
    "            reviewer_website = r.find('a', href=True)\n",
    "            reviewer_website = 'http://www.tripadvisor.com'+reviewer_website['href']\n",
    "            rating = r.find('span', class_=\"ui_bubble_rating\")\n",
    "            rating = str(rating).strip('\"0></span>')[-1:]\n",
    "            try:\n",
    "                date =  r.find(class_ = 'hotels-review-list-parts-EventDate__event_date--CRXs4').find('span').get_text()\n",
    "            except:\n",
    "                date = ''\n",
    "            \n",
    "            if reviewer_website not in list(reviewer_list['user_link']): # avoid duplicate reviewer\n",
    "                \n",
    "                df = pd.DataFrame({'uid':reviewer, 'user_link':reviewer_website, 'rating':int(rating), 'date_of_stay':date[14:], 'num_of_reviews':int(contribute)}, \n",
    "                                  columns=['uid', 'user_link', 'rating', 'date_of_stay', 'num_of_reviews'], index=[0])\n",
    "                reviewer_list = pd.concat([reviewer_list, df], axis =0)   \n",
    "                review_count -= 1\n",
    "            else:\n",
    "                pass\n",
    "    if next_page and review_count > 0:\n",
    "        t.sleep(1)\n",
    "        try:\n",
    "            response = requests.get(next_page)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            t.sleep(60) # Retrying same url after 1 minute\n",
    "            response = requests.get(next_page)\n",
    "            soup = BeautifulSoup(response.text)\n",
    "            reviews = soup.findAll(class_ = 'hotels-review-list-parts-SingleReview__reviewContainer--d54T4')\n",
    "            reviewer_list = iterate_review_page(soup, reviews, review_count, reviewer_list)\n",
    "        else:\n",
    "            soup = BeautifulSoup(response.text)\n",
    "            reviews = soup.findAll(class_ = 'hotels-review-list-parts-SingleReview__reviewContainer--d54T4')\n",
    "            reviewer_list = iterate_review_page(soup, reviews, review_count, reviewer_list)\n",
    "        finally:\n",
    "            return reviewer_list\n",
    "\n",
    "    return reviewer_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, hotel in enumerate(tahotel[0:1000]['hotel_url']):\n",
    "    hotel_url = hotel\n",
    "    print(index) # keeping track of progress in case of error\n",
    "    n = len(reviewer_list)\n",
    "    hotel_df = process_reviews(hotel_url, reviewer_list)\n",
    "    reviewer_list = pd.concat([reviewer_list, hotel_df[:][n:]], axis =0)\n",
    "    t.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_list.to_csv('tripadvisor_reviewer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping through TripAdvisor Guest Reviewer Webpage for Hotel Reviews using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_other_reviews(user_url, hotel_review_list):\n",
    "    \n",
    "    driver.get(user_url)\n",
    "    t.sleep(1.5)\n",
    "    try: # check if driver get user_url page\n",
    "        response = driver.page_source\n",
    "        hotel_review_list = try_button(driver, response, user_url, hotel_review_list)   \n",
    "    except: # retry getting user_url with Selenium\n",
    "        driver.get(user_url)\n",
    "        t.sleep(1.5)\n",
    "        try:\n",
    "            response = driver.page_source\n",
    "        except Exception as e: # print reason for why it fails\n",
    "            print(e)\n",
    "        else: # check if 'show more' button is clickable\n",
    "            hotel_review_list = try_button(driver, response, user_url, hotel_review_list)\n",
    "            \n",
    "    return hotel_review_list\n",
    " \n",
    "\n",
    "def try_button(driver, response, user_url, hotel_review_list):   # check if 'show more' button is clickable\n",
    "    \n",
    "    try:\n",
    "        python_button = driver.find_element_by_class_name(\"social-show-more-ShowMore__button_contents--1djai\")\n",
    "        python_button.click()\n",
    "        soup = BeautifulSoup(response)\n",
    "        hotel_review_list = iterate_reviewer_page(soup, user_url, hotel_review_list)\n",
    "    except:\n",
    "        soup = BeautifulSoup(response)\n",
    "        hotel_review_list = iterate_reviewer_page(soup, user_url, hotel_review_list)\n",
    "        \n",
    "    return hotel_review_list\n",
    "\n",
    "    \n",
    "def iterate_reviewer_page(soup, user_url, hotel_review_list):    # filter out reviews that are not of hotels from the 16 european cities \n",
    "    \n",
    "    reviews = soup.findAll('div', class_ = 'social-sections-CardSection__card_section--3Hc9Y ui_card section')\n",
    "    for r in reviews:\n",
    "        hotel = r.find('div', class_=\"social-sections-POICarousel__container--297jy social-sections-POICarousel__carousel--1vz03\").find('a', href=True)\n",
    "        if not hotel:\n",
    "            continue\n",
    "        try:\n",
    "            hotel = hotel['href']    # grab website link info\n",
    "        except:\n",
    "            continue\n",
    "        if \"Hotel_Review\" in hotel:    # check if it is a hotel review and from the 16 cities\n",
    "            if 'London_England.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Paris_lle_de_France.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Rome_Lazio.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Barcelona_Catalonia.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Berlin.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Vienna.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Prague_Bohemia.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Budapest_Central_Hungary.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Athens_Attica.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Florence_Tuscany.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Milan_Lombardy.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Madrid.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Lisbon_District_Central_Portugal.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Edinburgh_Scotland.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Amsterdam_North_Holland_Province.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            elif 'Brussels.html' in hotel:\n",
    "                hotel_review_list = scrape_reviews(r, user_url, hotel_review_list, hotel)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    return hotel_review_list\n",
    "\n",
    "\n",
    "def scrape_reviews(r, user_url, hotel_review_list, hotel):\n",
    "    \n",
    "    reviewer = r.find('a', class_ = 'ui_link social-member-event-MemberEventOnObjectBlock__member--35-jC').get_text()\n",
    "    try:\n",
    "        title = r.find('div', class_=\"social-sections-ReviewSection__title--35ISZ social-sections-ReviewSection__linked--2rTun\").get_text()\n",
    "    except:\n",
    "        title = ''\n",
    "    try:\n",
    "        preview = r.find('q', class_=\"social-sections-ReviewSection__quote--3gE7d\").get_text()\n",
    "    except:\n",
    "        preview = ''\n",
    "    rating = r.find('span', class_=\"ui_bubble_rating\")\n",
    "    rating = str(rating).strip('\"0></span>')[-1:]\n",
    "    hotel = 'http://www.tripadvisor.com' + hotel\n",
    "    try:\n",
    "        date =  r.find('div', class_ = 'social-review-info-EventDate__event_date--2d3vn').find('span').get_text()\n",
    "    except:\n",
    "        date = ''\n",
    "    df = pd.DataFrame({'uid':reviewer, 'user_link':user_url, 'rating':float(rating), 'hotel_link':hotel, 'date_of_stay':date[14:], 'title':title, 'review_preview':preview}, \n",
    "                                  columns=['uid', 'user_link', 'rating', 'hotel_link', 'date_of_stay', 'title', 'review_preview'], index=[0])\n",
    "    hotel_review_list = pd.concat([hotel_review_list, df])   \n",
    "    hotel_review_list = hotel_review_list.reset_index(drop=True)\n",
    "    return hotel_review_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_review_list = pd.DataFrame(columns=['uid', 'user_link', 'rating', 'hotel_link', 'date_of_stay', 'title', 'review_preview'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tzulungs/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: use options instead of chrome_options\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument(\"--test-type\")\n",
    "driver = webdriver.Chrome(chrome_options=options)\n",
    "# driver = webdriver.Chrome(\"chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bfde12849f08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_url\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviewer_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_link\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhotel_review_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_other_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhotel_review_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-8c4207ce5211>\u001b[0m in \u001b[0;36mprocess_other_reviews\u001b[0;34m(user_url, hotel_review_list)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_other_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhotel_review_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# check if driver get user_url page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \"\"\"\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/urllib3/request.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     70\u001b[0m             return self.request_encode_body(method, url, fields=fields,\n\u001b[1;32m     71\u001b[0m                                             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                                             **urlopen_kw)\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     def request_encode_url(self, method, url, fields=None, headers=None,\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/urllib3/request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, user_url in enumerate(reviewer_list.user_link[0:10000]):\n",
    "    print(index)\n",
    "    hotel_review_list = process_other_reviews(user_url, hotel_review_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning And Preliminary Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadata = pd.read_csv('tripadvisor_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning 'reviews' column and reassign column to integer type\n",
    "tadata['reviews'].replace('1 review', '1', inplace=True)\n",
    "tadata['reviews'] = tadata.reviews.astype('int64')\n",
    "\n",
    "# dropping 'tripadvisor_rating' that are less than 3.0\n",
    "tadata = tadata.drop(tadata[tadata['tripadvisor_rating'] < 3.0].index)\n",
    "\n",
    "# cleaning 'price_per_night' column and reassign column to integer type\n",
    "tadata['price_per_night'] = tadata['price_per_night'].apply(lambda x: str(x).strip('$'))\n",
    "tadata['price_per_night'] = tadata['price_per_night'].apply(lambda x: str(x).replace(',', ''))\n",
    "tadata = tadata.drop(tadata.loc[tadata['price_per_night']=='nan'].index)\n",
    "tadata['price_per_night'] = tadata.price_per_night.astype('int64')\n",
    "\n",
    "print(tadata.info())\n",
    "print(tadata.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few nan values in tripadvisor_rating. Since having a rating is necessary for the recommender system, the rows with the missing rating will be dropped. The distribution of 'price_per_night' is skewed and taking a log function is warranted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadata = tadata[np.isfinite(tadata['tripadvisor_rating'])]\n",
    "tadata['log_ppp'] = tadata.price_per_night.apply(lambda x: np.log(x))\n",
    "tadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15, 8)})\n",
    "box_fig = sns.boxplot(x=\"locality\", y=\"log_ppp\", data=tadata)\n",
    "box_fig.set_xticklabels(tadata.locality.unique(), rotation=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadata1 = tadata.groupby(['locality', 'tripadvisor_rating'])\n",
    "grouped1 = tadata1['price_per_night'].agg(np.mean).round(2)\n",
    "grouped1 = grouped1.unstack(level=-1)\n",
    "grouped1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_barplot1 = sns.catplot(x=\"locality\", y=\"log_ppp\", hue=\"tripadvisor_rating\", data=tadata,\n",
    "                height=6, kind=\"bar\", size=5, aspect=4, palette=\"muted\")\n",
    "group_barplot1.despine(left=True)\n",
    "group_barplot1.set_ylabels(\"log value of price_per_night\")\n",
    "group_barplot1.set_xticklabels(tadata.locality.unique(), rotation=70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped2 = tadata1['hotel_name'].count()\n",
    "grouped2 = grouped2.unstack(level=-1)\n",
    "grouped2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped2.plot.bar(figsize=(15, 6), cmap=plt.cm.rainbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tadata2 = tadata.groupby([\"tripadvisor_rating\", pd.cut(tadata[\"no_of_deals\"], np.arange(0, 25, 5))])\n",
    "grouped3 = tadata2['price_per_night'].agg(np.mean).round(2)\n",
    "grouped3 = grouped3.unstack(level=1)\n",
    "print(grouped3)\n",
    "grouped3.plot.bar(figsize=(10, 6), cmap=plt.cm.rainbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tahotel = pd.read_json('tripadvisor_hotel.json')\n",
    "tahotel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning tripadvisor_hotel.json\n",
    "tahotel = pd.concat([tahotel.drop(['address'], axis=1), tahotel['address'].apply(pd.Series)], axis=1)\n",
    "tahotel.drop('region', axis=1, inplace=True)\n",
    "\n",
    "# make new columns from the dictionary of values in the ratings column and converting it to integers\n",
    "tahotel = pd.concat([tahotel.drop(['ratings'], axis=1), tahotel['ratings'].apply(pd.Series)], axis=1)\n",
    "tahotel['Excellent'] = tahotel['Excellent'].apply(lambda x: str(x).replace(',', ''))\n",
    "tahotel['Good'] = tahotel['Good'].apply(lambda x: str(x).replace(',', ''))\n",
    "tahotel['Average'] = tahotel['Average'].apply(lambda x: str(x).replace(',', ''))\n",
    "tahotel['Poor'] = tahotel['Poor'].apply(lambda x: str(x).replace(',', ''))\n",
    "tahotel['Terrible'] = tahotel['Terrible'].apply(lambda x: str(x).replace(',', ''))\n",
    "\n",
    "tahotel['Excellent'] = tahotel.Excellent.astype('int64', errors='ignore')\n",
    "tahotel['Good'] = tahotel.Good.astype('int64', errors='ignore')\n",
    "tahotel['Average']= tahotel.Average.astype('int64', errors='ignore')\n",
    "tahotel['Poor'] = tahotel.Poor.astype('int64', errors='ignore')\n",
    "tahotel['Terrible'] = tahotel.Terrible.astype('int64', errors='ignore')\n",
    "\n",
    "# get dummy variables from amenities column\n",
    "tahotel = pd.concat([tahotel.drop('amenities', axis=1), pd.get_dummies(tahotel['amenities'].apply(pd.Series).stack()).sum(level=0)], axis=1)\n",
    "\n",
    "tahotel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_hotel_reviews = pd.read_csv('tripadvisor_hotel_review.csv')\n",
    "combined_hotel_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique ids for hotels and guest reviewers\n",
    "combined_hotel_reviews = combined_hotel_reviews.assign(uid=(combined_hotel_reviews['user_link']).astype('category').cat.codes)\n",
    "combined_hotel_reviews = combined_hotel_reviews.assign(hid=(combined_hotel_reviews['hotel_link']).astype('category').cat.codes)\n",
    "# dropping off 3 columns\n",
    "combined_hotel_reviews.drop(combined_hotel_reviews[['date_of_stay', 'title', 'review_preview']], axis=1, inplace=True)\n",
    "combined_hotel_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_hotel_reviews['hotel_link'].isin(tahotel['hotel_url']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hotels = combined_hotel_reviews[~combined_hotel_reviews['hotel_link'].isin(tahotel['hotel_url']).dropna()]\n",
    "len(new_hotels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_hotel_reviews = combined_hotel_reviews.drop(combined_hotel_reviews.index[13328])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_hotel_reviews = combined_hotel_reviews.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_hotel_reviews = pd.merge(combined_hotel_reviews,\n",
    "                 tahotel[['hotel_url', 'name', 'street_address', 'locality', 'country', 'hotel_rating']],\n",
    "                 left_on='hotel_link',\n",
    "                 right_on='hotel_url',\n",
    "                 how='left')\n",
    "combined_hotel_reviews = combined_hotel_reviews.drop(combined_hotel_reviews[['hotel_url']], axis=1)\n",
    "combined_hotel_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System with Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_combined_reviews = combined_hotel_reviews.groupby(['hid', 'uid'])\n",
    "grouped_rating = grouped_combined_reviews[['rating']].agg(np.mean).round(2)\n",
    "grouped_rating.reset_index(inplace=True)\n",
    "grouped_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(grouped_rating[['uid', 'hid', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    \n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)\n",
    "    \n",
    "pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_result = pd.DataFrame(columns=['algorithm', 'param_grid', 'best_rmse_score', 'best_params', 'test_prediction_rmse', 'fit_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using SVDpp')\n",
    "\n",
    "n_epochs = [15, 20, 25]\n",
    "lr_all = [0.005, 0.01, 0.015]\n",
    "reg_all = [0.3, 0.4, 0.5]\n",
    "\n",
    "param_grid = {'n_epochs': n_epochs, 'lr_all': lr_all, 'reg_all': reg_all}\n",
    "gs1 = GridSearchCV(SVDpp, param_grid, measures=['rmse', 'mae'], cv=5)\n",
    "# t0 = t.time()\n",
    "gs1.fit(data)\n",
    "# t1 = t.time()\n",
    "# lapsed = t1-t0\n",
    "\n",
    "print(gs1.best_score['rmse'])\n",
    "print(gs1.best_params['rmse'])\n",
    "\n",
    "algo = gs1.best_estimator['rmse']\n",
    "predictions1 = algo.fit(trainset).test(testset)\n",
    "accuracy.rmse(predictions1)\n",
    "\n",
    "# grid_search_result = grid_search_result.append({'algorithm':'SVDpp', 'param_grid':{'n_epochs':n_epochs, 'lr_all':lr_all, 'reg_all':reg_all}, 'best_rmse_score':gs1.best_score['rmse'], 'best_params':gs1.best_params['rmse'], 'test_prediction_rmse':accuracy.rmse(predictions1), 'fit_time':lapsed}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KNN Baseline Using ALS and SGD')\n",
    "\n",
    "method = ['als', 'sgd']\n",
    "reg = [1, 2]\n",
    "k = [2, 3]\n",
    "name = ['msd', 'cosine']\n",
    "min_support = [1, 5, 10]\n",
    "user_based = [False]\n",
    "\n",
    "param_grid = {'bsl_options': {'method':method, 'reg':reg}, 'k':k, 'sim_options': {'name':name, 'min_support':min_support, 'user_based':user_based}}\n",
    "gs2 = GridSearchCV(KNNBaseline, param_grid, measures=['rmse', 'mae'], cv=5)\n",
    "t0 = t.time()\n",
    "gs2.fit(data)\n",
    "t1 = t.time()\n",
    "lapsed = t1-t0\n",
    "\n",
    "print(gs2.best_score['rmse'])\n",
    "print(gs2.best_params['rmse'])\n",
    "\n",
    "algo = gs2.best_estimator['rmse']\n",
    "predictions2 = algo.fit(trainset).test(testset)\n",
    "accuracy.rmse(predictions2)\n",
    "\n",
    "# grid_search_result = grid_search_result.append({'algorithm':'KNNBaseline', 'param_grid':{'method':method, 'reg':reg}, 'k':k, 'sim_options': {'name':name, 'min_support':min_support, 'user_based':user_based}, 'best_rmse_score':gs2.best_score['rmse'], 'best_params':gs2.best_params['rmse'], 'test_prediction_rmse':accuracy.rmse(predictions2), 'fit_time':lapsed}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using SVD')\n",
    "\n",
    "n_epochs = [20, 30, 40]\n",
    "lr_all = [0.005, 0.01, 0.02]\n",
    "reg_all = [0.3, 0.4, 0.5]\n",
    "\n",
    "param_grid = {'n_epochs':n_epochs, 'lr_all':lr_all, 'reg_all':reg_all}\n",
    "gs3 = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=5)\n",
    "t0 = t.time()\n",
    "gs3.fit(data)\n",
    "t1 = t.time()\n",
    "lapsed = t1-t0\n",
    "\n",
    "print(gs3.best_score['rmse'])\n",
    "print(gs3.best_params['rmse'])\n",
    "\n",
    "algo = gs3.best_estimator['rmse']\n",
    "predictions3 = algo.fit(trainset).test(testset)\n",
    "accuracy.rmse(predictions3)\n",
    "\n",
    "# grid_search_result = grid_search_result.append({'algorithm':'SVD', 'param_grid':{'n_epochs':n_epochs, 'lr_all':lr_all, 'reg_all':reg_all}, 'best_rmse_score':gs3.best_score['rmse'], 'best_params':gs3.best_params['rmse'], 'test_prediction_rmse':accuracy.rmse(predictions3), 'fit_time':lapsed}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Baseline Only Using ALS and SGD')\n",
    "\n",
    "method = ['als', 'sgd']\n",
    "reg = [1, 2]\n",
    "k = [1, 2, 3]\n",
    "\n",
    "param_grid = {'bsl_options': {'method':method, 'reg':reg, 'k': k}}\n",
    "gs3 = GridSearchCV(BaselineOnly, param_grid, measures=['rmse', 'mae'], cv=5)\n",
    "t0 = t.time()\n",
    "gs3.fit(data)\n",
    "t1 = t.time()\n",
    "lapsed = t1-t0\n",
    "\n",
    "print(gs3.best_score['rmse'])\n",
    "print(gs3.best_params['rmse'])\n",
    "\n",
    "algo = gs3.best_estimator['rmse']\n",
    "predictions3 = algo.fit(trainset).test(testset)\n",
    "accuracy.rmse(predictions3)\n",
    "\n",
    "# grid_search_result = grid_search_result.append({'algorithm':'BaselineOnly', 'param_grid':{'method':method, 'reg':reg, 'k': k}, 'best_rmse_score':gs3.best_score['rmse'], 'best_params':gs3.best_params['rmse'], 'test_prediction_rmse':accuracy.rmse(predictions3), 'fit_time':lapsed}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump best algorithm and reload it.\n",
    "file_name = os.path.expanduser('surprise_SVD_model')\n",
    "dump.dump(file_name, algo=algo)\n",
    "_, loaded_algo = dump.load(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First train an SVD algorithm\n",
    "data = Dataset.load_from_df(grouped_rating[['uid', 'hid', 'rating']], reader)\n",
    "trainset = data.build_full_trainset()\n",
    "algo = loaded_algo\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env]",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
